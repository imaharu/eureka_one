# eurekaインターン前半課題

## 目次

- テーマ設定
- 各種ワードの説明
- 方法
- 計測条件
- デフォルト結果
- チューニング
- 改善点
- 次にやりたいこと

## テーマ設定

各文庫の50行のデータを特徴量として、出版社を分類する。
今回は、岩波書店か、ちくま文庫の分類問題である。



## Stopword



## Bag of Words

## TF-IDF



## LSI(潜在的意味インデキシング)

高次元の行列データを次元圧縮するための手法

LSIの特徴として、出現頻度の低いものから指定した次元数基づき次元を圧縮する。

テーマに関して、出版社によって関連ワードがあると思ったためLSIを選定した。

[潜在的意味インデキシング（LSI）徹底入門](https://abicky.net/2012/03/24/211818/)

## 方法

### データ整形
Stopwordを用いて、

### モデル

## 計測条件

bag of wordsとTF-IDF組み合わせ文章の分散表現を得る

LSIで次元の圧縮を行い、200次元にする

データの標準化を行う

訓練データ: テストデータ = 8 : 2

100回の平均値をとる

全ての学習モデルはパラメータは公式のサイトのサンプルコードとする


## シンプルな結果

### サポートベクターマシン

データ数 814
```
正解率 0.853086419753
精度 0.827951029892
検出率 0.889742940214
F値 0.856738459335
```

データ数 1710
```
正解率 0.894017595308
精度 0.898894259526
検出率 0.911991241483
F値 0.905122037767
```

C=1.5
```
正解率 0.898093841642
精度 0.902793828739
検出率 0.914153969662
F値 0.9081699341
```

### ランダムフォレスト

データ数 814
```
正解率 0.764320987654
精度 0.797706546766
検出率 0.700258606916
F値 0.743572530543
```

データ数 1710
```
正解率 0.753460410557
精度 0.74482934547
検出率 0.838209324053
F値 0.787985012345
```

### 確率的勾配降下法

データ数 814
```
正解率 0.850740740741
精度 0.85359337175
検出率 0.842762287573
F値 0.847312487853
```

データ数 1710
```
正解率 0.860791788856
精度 0.876019391374
検出率 0.873612206428
F値 0.874361513606
```

## チューニング結果

```
linear
------------------
正解率 0.866568914956
精度 0.887514073488
検出率 0.866718675909
F値 0.876694894379
------------------
poly
------------------
正解率 0.791700879765
精度 0.940843856684
検出率 0.673149892024
F値 0.775105811495
------------------
rbf
------------------
正解率 0.898885630499
精度 0.90469428534
検出率 0.912423391024
F値 0.908262177742
------------------
sigmoid
------------------
正解率 0.851260997067
精度 0.867847000912
検出率 0.861240922296
F値 0.86410664437
------------------
```


## 改善点

### 1
現在、データのカット処理は出現回数が4以下、全データのうち30%以上を含むを条件として行っている。
各データの単語の出現回数を調べ、共通して出現する単語が出現頻度が高い場合はStopwordに含めると、よりデータ間の違いが際立つのではないかと考える。

### 2


## 次にやりたいこと

- word2vecなど単語の位置情報を考慮した方法を試し、精度の差をみる

- 多値分類の結果をみる

- 各種パラーメータ変更によってなぜ、精度が変化したのか調べる

- GPUを使いたいので、DeepLearningをやりたい

## 感想

データを整形するのに最も、時間がかかるのを体感した。


## tips

[StopWordの除去](http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt)

complete.corpus*で削除した文字
\u3000

cat utf8_delete_over50.txt | grep -f iwanami.url
cat test.txt | rev | cut  -f 1 -d "," | rev
cat utf8_delete_over50.txt | grep -f iwanami.url >> utf8_iwanami.txt